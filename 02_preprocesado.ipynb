{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZR4xF2iQ661kOIFEJLLdX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clemgi0/movie-analyser_deep-learning-proyecto/blob/main/02_preprocesado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Movie Analyser | Deep Learning Final Project\n",
        "\n",
        "In this serie of notebook, we will follow my avances for this project. Let's begin by defining it. Basically, what I want to achieve is to create a deep learning AI model using Keras and Tensorflow that could predict the success of a movie through it's resume, and some other possible input datas like the name of the movie, it's director or it's genre.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-TXelVReUzrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATASETS\n",
        "https://www.kaggle.com/datasets/harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows\n",
        "\n",
        "Features of the first dataset :\n",
        "\n",
        "Here are it's features:\n",
        "\n",
        "0 Poster_Link - Link of the poster that imdb using\n",
        "\n",
        "1 Series_Title - Name of the movie\n",
        "\n",
        "2 Released_Year - Year at which that movie released\n",
        "\n",
        "3 Certificate - Certificate earned by that movie\n",
        "\n",
        "4 Runtime - Total runtime of the movie\n",
        "\n",
        "5 Genre - Genre of the movie\n",
        "\n",
        "6 IMDB_Rating - Rating of the movie at IMDB site\n",
        "\n",
        "7 Overview - mini story/ summary\n",
        "\n",
        "8 Meta_score - Score earned by the movie\n",
        "\n",
        "9 Director - Name of the Director\n",
        "\n",
        "10, 11, 12, 13 Star1,Star2,Star3,Star4 - Name of the Stars\n",
        "\n",
        "14 No_of_votes - Total number of votes\n",
        "\n",
        "15 Gross - Money earned by that movie\n",
        "\n"
      ],
      "metadata": {
        "id": "yJ7u1doGF7A3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/datasets/stefanoleone992/filmtv-movies-dataset\n",
        "\n",
        "Features of the second dataset :\n",
        "\n",
        "0 Filmtv_id - Movie id\n",
        "\n",
        "1 Title - Name of the movie\n",
        "\n",
        "2 Year - Movie year of release\n",
        "\n",
        "3 Genre - Movie genre\n",
        "\n",
        "4 Duration - Movie duration (in min)\n",
        "\n",
        "5 Country - Countries where the movie was filmed\n",
        "\n",
        "6 Directors - Name of movie directors\n",
        "\n",
        "7 Actors - Name of movie actors\n",
        "\n",
        "8 Avg_vote - Average rating (by critics and public)\n",
        "\n",
        "9 Critics_vote - Average vote of the critics\n",
        "\n",
        "10 Public_vote - Average vote of the public\n",
        "\n",
        "11 Total_vote - Total votes expressed by critics and public\n",
        "\n",
        "12 Overview - Movie description\n",
        "\n",
        "13 Notes - Movie notes\n",
        "\n",
        "14 Humor - Movie humor score given by filmtv\n",
        "\n",
        "15 Rythm - Movie rythm score given by filmtv\n",
        "\n",
        "16 Tension - Movie tension score given by filmtv\n",
        "\n",
        "17 Erotism - Movie erotism score given by filmtv"
      ],
      "metadata": {
        "id": "4pFZ7v1hGFmi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7cnlQ6HsIx7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import os\n",
        "import nltk\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess of the data\n",
        "In this second notebook, we will focus on the preprocessing of the data that we have collected and explored previously."
      ],
      "metadata": {
        "id": "dKNgMiSLF856"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we import the datas we want and shuffle them for the reason we saw on the first notebook. We also withdraw the features that interests us."
      ],
      "metadata": {
        "id": "EPHKiUT-XyFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we select the dataset that we want to use.**"
      ],
      "metadata": {
        "id": "jrC2c9DXpKh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows\")\n",
        "dataset = \"IMDB\""
      ],
      "metadata": {
        "id": "nFHSBmieLDL0",
        "outputId": "aa950e95-82fa-4556-c3cf-ddd62de09c5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'imdb-dataset-of-top-1000-movies-and-tv-shows' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"stefanoleone992/filmtv-movies-dataset\")\n",
        "dataset = \"FilmTV\""
      ],
      "metadata": {
        "id": "teoTHoALLDu5",
        "outputId": "82307cd5-fc95-4048-a3e7-dd803d73c473",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'filmtv-movies-dataset' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the IMDB dataset, we will calculate the average score later on, here we put the Release year as a dump value."
      ],
      "metadata": {
        "id": "XxACjkfo7loQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files_in_path = os.listdir(path)\n",
        "csv_files = [f for f in files_in_path if f.endswith('.csv')]\n",
        "\n",
        "if csv_files:\n",
        "    data_file = os.path.join(path, csv_files[0])\n",
        "    df = pd.read_csv(data_file)\n",
        "\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True) # Shuffle the datas to avoid linear IMDB rating\n",
        "\n",
        "    data = df.to_numpy()\n",
        "    if dataset == \"IMDB\":\n",
        "      data = data[:, [1, 5, 7, 9, 2, 6, 8]] # Title / Genre / Overview / Director / Average score / IMDB rating / meta-score\n",
        "    else:\n",
        "      data = data[:, [1, 3, 12, 6, 8]] # Title / Genre / Overview / Director / Average votes\n",
        "\n",
        "    print(\"Data shape:\", data[:3,:])\n",
        "else:\n",
        "    print(\"No CSV files found in the specified path. Please specify which file to load if it's not a CSV or has a different extension.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo6Y6h7JUmId",
        "outputId": "60fd71ec-6360-4199-f00b-548369bf2a20",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: [['Svadba' 'Comedy'\n",
            "  'Mishka and Tania, friends since school, are getting married. But something is wrong because the girl leaves for Moscow and disappears for a few years. Having vanished the dreams of becoming a model, she decides to return to the country to marry the good Mishka, muscular and solid worker with a clean face, still in love with her. At this point the film tells about the wedding preparations, the ceremony, the dramas and the trafficking that goes through it.'\n",
            "  'Pavel Lungin' 7.0]\n",
            " ['The Phantom of Crestwood' 'Thriller'\n",
            "  'Pushed by the beautiful Jenny Wren, banker Priam Andes throws a party at Crestwood, his summer residence. The girl asks Priam to also invite three wealthy men whom she intends to pluck, but her plans will be unexpectedly upset by an inexplicable death ...'\n",
            "  'J. Walter Ruben' 6.5]\n",
            " ['Ragazzi della marina' 'War'\n",
            "  'The cruiser \"Raimondo Montecuccoli\" leaves Livorno with the cadets of the Naval Academy. Among them, three sailors are worried about problems of a different nature. The authority of the superiors and the common life on board improves the character of the crew, also managing to solve their personal problems.'\n",
            "  'Francesco De Robertis' 6.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we get rid of the rows where one of the features selected have a nan value."
      ],
      "metadata": {
        "id": "ewXIsc4ZpfSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.DataFrame(data)\n",
        "df2 = df2.replace(\"nan\", np.nan)   # if \"nan\" is a chain, we delete the row\n",
        "df2 = df2.dropna()\n",
        "data_clean = df2.to_numpy()"
      ],
      "metadata": {
        "id": "NzDXfdCbTULH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xkObqEo2uuQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "stopwords_en = nltk.corpus.stopwords.words('english')\n",
        "stopwords_all = stopwords_en\n",
        "stopwords_all += nltk.corpus.stopwords.words('spanish')\n",
        "stopwords_all += nltk.corpus.stopwords.words('french')\n",
        "stopwords_all += nltk.corpus.stopwords.words('italian')\n",
        "stopwords_all += nltk.corpus.stopwords.words('german')\n",
        "\n",
        "stopwords_all = set(stopwords_all)\n",
        "\n",
        "def remove_stopwords(text_list, language): # For the Overview since they are only in English\n",
        "    cleaned_texts = []\n",
        "    for text in text_list:\n",
        "      if language == \"english\":\n",
        "        tokens = [word.lower() for word in nltk.word_tokenize(text) if word.lower() not in stopwords_en]\n",
        "      else:\n",
        "        tokens = [word.lower() for word in nltk.word_tokenize(text) if word.lower() not in stopwords_all]\n",
        "      cleaned_texts.append(' '.join(tokens))\n",
        "    return cleaned_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-tBaID1e7cd",
        "outputId": "93312790-b688-4b92-bc28-a05c6d099960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing the Overview"
      ],
      "metadata": {
        "id": "ZwVaKPK7rVt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After downloading the english stopwords from nltk, we use them to remove them from our Overview feature to only keep the meaningful words (if the cell below fails, re-run the downloading cell of the dataset and the following ones)."
      ],
      "metadata": {
        "id": "kEXwrIUiYJlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean[:,2] = remove_stopwords(data_clean[:,2], \"english\")\n",
        "print(\"First 3 rows of the cleaned Overviews:\\n\\n\",data_clean[:3,2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4foS7os6e8Qb",
        "outputId": "2d0173fc-7d23-4951-81a2-1591af6f454b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 rows of the cleaned Overviews:\n",
            "\n",
            " ['mishka tania , friends since school , getting married . something wrong girl leaves moscow disappears years . vanished dreams becoming model , decides return country marry good mishka , muscular solid worker clean face , still love . point film tells wedding preparations , ceremony , dramas trafficking goes .'\n",
            " 'pushed beautiful jenny wren , banker priam andes throws party crestwood , summer residence . girl asks priam invite three wealthy men intends pluck , plans unexpectedly upset inexplicable death ...'\n",
            " 'cruiser `` raimondo montecuccoli `` leaves livorno cadets naval academy . among , three sailors worried problems different nature . authority superiors common life board improves character crew , managing solve personal problems .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we tokenize our Overviews with a max_features of 5000 or 50000 given that there is 5060 different words in our IMDB dictionnary and 60064 in our FilmTV dictionnary. We might want to reduce this max_feature to keep only the most used words if by doing this our model accuracy gets better."
      ],
      "metadata": {
        "id": "JM5MKPvUZjyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "if dataset == \"IMDB\":\n",
        "  ov_max_features = 5000\n",
        "else:\n",
        "  ov_max_features = 50000\n",
        "\n",
        "ov_tokenizer = Tokenizer(num_words=ov_max_features, split=' ')\n",
        "ov_tokenizer.fit_on_texts(data_clean[:,2])\n",
        "ov_tokenizer.word_index.update({'<pad>': 0})\n",
        "ov_tokenized = ov_tokenizer.texts_to_sequences(data_clean[:,2])\n",
        "\n",
        "print(\"First 3 rows of the cleaned Overview:\\n\", data_clean[:,2][:3])\n",
        "print(\"\\nFirst 3 rows of the tokenized Overview:\\n\", ov_tokenized[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T-qU7ngijsw",
        "outputId": "83461192-4762-43a2-9906-2fedcc8af945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 rows of the cleaned Overview:\n",
            " ['mishka tania , friends since school , getting married . something wrong girl leaves moscow disappears years . vanished dreams becoming model , decides return country marry good mishka , muscular solid worker clean face , still love . point film tells wedding preparations , ceremony , dramas trafficking goes .'\n",
            " 'pushed beautiful jenny wren , banker priam andes throws party crestwood , summer residence . girl asks priam invite three wealthy men intends pluck , plans unexpectedly upset inexplicable death ...'\n",
            " \"cruiser `` raimondo montecuccoli '' leaves livorno cadets naval academy . among , three sailors worried problems different nature . authority superiors common life board improves character crew , managing solve personal problems .\"]\n",
            "\n",
            "First 3 rows of the tokenized Overview:\n",
            " [[28433, 9714, 34, 149, 65, 646, 92, 191, 556, 16, 169, 2223, 878, 13, 10929, 230, 403, 914, 28, 93, 165, 268, 84, 28433, 8857, 2430, 745, 3320, 110, 131, 7, 208, 19, 418, 303, 3929, 2915, 5181, 1203, 78], [2269, 60, 2043, 18758, 2447, 20854, 11427, 1858, 288, 37378, 342, 2316, 16, 346, 20854, 3875, 30, 333, 96, 1197, 37379, 444, 1249, 875, 2520, 37], [18759, 11428, 37380, 6, 169, 8114, 10492, 4570, 2772, 130, 30, 4317, 2191, 227, 122, 538, 4571, 2296, 885, 5, 1077, 10930, 439, 899, 2881, 870, 436, 227]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of unique words in the vocabulary\n",
        "unique_words_count = len(ov_tokenizer.word_index) - 1 # Subtract 1 for the <pad> token\n",
        "print(f\"The number of different words in the\", dataset, \"dataset is:\", unique_words_count)\n",
        "print(\"\\nList of some of the words that have been tokenized from the Overview:\\n\", list(ov_tokenizer.word_index)[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrhnQmnYnNjl",
        "outputId": "26576557-791b-4cd3-a4fa-0420f3d5da34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of different words in the FilmTV dataset is: 63444\n",
            "\n",
            "List of some of the words that have been tokenized from the Overview:\n",
            " [\"'s\", 'two', 'one', 'young', 'life', \"''\", 'love', 'however', 'old', 'new', 'family', 'woman', 'years', 'father', 'time', 'girl', 'world', 'wife', 'film', 'find']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing the Title"
      ],
      "metadata": {
        "id": "HEfukuvJrc3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will tokenize the Title feature. Since the titles are in differents languages in the FilmTV dataset, we will clean them from their stopwords from all of the main languages that are present (English, Italian, French, German and Spanish). Just like before, we set the max feature just a bit beneath the total amount of words in the dictionnary for each dataset (after removing the stopwords: IMDB 1326 words / FilmTV 26153 words)."
      ],
      "metadata": {
        "id": "8PQ5XdQ5yFuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean[:,0] = remove_stopwords(data_clean[:,0], \"all\")\n",
        "print(\"First 3 rows of the cleaned Titles:\\n\\n\",data_clean[:3,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0FMBgGRu_5i",
        "outputId": "26d719f9-a435-48a6-9c53-afef8e88ec5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 rows of the cleaned Titles:\n",
            "\n",
            " ['svadba' 'phantom crestwood' 'ragazzi marina']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset == \"IMDB\":\n",
        "  ti_max_features = 1000\n",
        "else:\n",
        "  ti_max_features = 25000\n",
        "\n",
        "ti_tokenizer = Tokenizer(num_words=ti_max_features, oov_token=\"<UNK>\")\n",
        "ti_tokenizer.fit_on_texts(data_clean[:,0])\n",
        "ti_tokenized = ti_tokenizer.texts_to_sequences(data_clean[:,0])\n",
        "\n",
        "print(\"First 3 rows of the Title:\\n\", data_clean[:,0][:3])\n",
        "print(\"\\nFirst 3 rows of the tokenized Title:\\n\", ti_tokenized[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv9Jd9R8rgR5",
        "outputId": "544d071a-932d-456f-d67a-7976670289db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 rows of the Title:\n",
            " ['svadba' 'phantom crestwood' 'ragazzi marina']\n",
            "\n",
            "First 3 rows of the tokenized Title:\n",
            " [[10099], [560, 10100], [561, 2678]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see that potter or 2 both appear a lot of time and we can have some good expactations that these informations would provide valuable information to our model.  "
      ],
      "metadata": {
        "id": "WgkG0FSyy09l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of unique words in the vocabulary\n",
        "unique_words_count = len(ti_tokenizer.word_index) - 1 # Subtract 1 for the <pad> token\n",
        "print(f\"The number of different words in the\", dataset, \"dataset is:\", unique_words_count)\n",
        "print(\"\\nList of some of the words that have been tokenized from the Title:\\n\", list(ti_tokenizer.word_index)[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9XzcQG-uSbR",
        "outputId": "89adf787-f000-4c02-d4c7-2d3eeb83e94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of different words in the FilmTV dataset is: 28448\n",
            "\n",
            "List of some of the words that have been tokenized from the Title:\n",
            " ['<UNK>', \"'s\", '2', 'love', 'christmas', 'night', 'last', 'dead', 'story', \"'\", 'ii', 'one', 'life', '3', 'girl', 'house', 'little', 'black', 'time', 'death']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Labeling the Director"
      ],
      "metadata": {
        "id": "L-cRWZYDzI_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will label the Director feature. Since a full name can't taken into two separates name because that would mean nothing, we will labelise the directors according to their name. There is no need to remove the stop words here since we only will have names. The number of different directors for IMDB dataset is 843 and for the FilmTV dataset is 13185."
      ],
      "metadata": {
        "id": "-aVeQ5Vj0gSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "directors_raw = data_clean[:, 3]\n",
        "\n",
        "le_director = LabelEncoder()\n",
        "di_labelized = le_director.fit_transform(directors_raw)"
      ],
      "metadata": {
        "id": "TvUrlLASzUXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of unique words in the vocabulary\n",
        "print(f\"The number of different words in the\", dataset, \"dataset is:\", len(di_labelized))\n",
        "print(\"\\nList of some of the words that have been tokenized from the Directors:\\n\", directors_raw[:10], \"\\nfor\\n\", di_labelized[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYCE4iBmzu7Y",
        "outputId": "4eb49b1f-733b-415d-a948-fc764837966a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of different words in the FilmTV dataset is: 39742\n",
            "\n",
            "List of some of the words that have been tokenized from the Directors:\n",
            " ['Pavel Lungin' 'J. Walter Ruben' 'Francesco De Robertis' 'Keoni Waxman'\n",
            " 'Francesco Invernizzi' 'Aamir Khan' 'Elie Chouraqui' 'Tara Wood'\n",
            " 'Paolo Sorrentino' 'Sam Pillsbury'] \n",
            "for\n",
            " [10798  6033  4409  7779  4422    14  3786 13262 10594 12262]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Labeling the Genre"
      ],
      "metadata": {
        "id": "02mgw-Og3TRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will first split the genres to get the list of genres of the movie since in the IMDB dataset there can be multiple genres. Then we tokenize it and select all of them in the tokenizer since there is a really small quantity different genre (23 in the IMDB dataset and 34 in the FilmTV dataset)."
      ],
      "metadata": {
        "id": "OFRQBiEb3-7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genres_split = [g.lower().split(\", \") for g in data_clean[:,1]]"
      ],
      "metadata": {
        "id": "yezEa_3v3Via"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset == \"IMDB\":\n",
        "  ge_max_features = 23\n",
        "else:\n",
        "  ge_max_features = 34\n",
        "\n",
        "ge_tokenizer = Tokenizer(num_words=ge_max_features, oov_token=\"<UNK>\")\n",
        "ge_tokenizer.fit_on_texts([\" \".join(g) for g in genres_split])\n",
        "ge_tokenized = ge_tokenizer.texts_to_sequences([\" \".join(g) for g in genres_split])\n"
      ],
      "metadata": {
        "id": "HhrwsjBT3Xjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of unique words in the vocabulary\n",
        "unique_words_count = len(ge_tokenizer.word_index) - 1 # Subtract 1 for the <pad> token\n",
        "print(f\"The number of different words in the\", dataset, \"dataset is:\", unique_words_count)\n",
        "print(\"\\nList of some of the words that have been tokenized from the Title:\\n\", list(ge_tokenizer.word_index)[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0z8AOVt3oEJ",
        "outputId": "667fa62b-4a70-4eb0-a9f2-b7549756f402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of different words in the FilmTV dataset is: 34\n",
            "\n",
            "List of some of the words that have been tokenized from the Title:\n",
            " ['<UNK>', 'drama', 'comedy', 'thriller', 'horror', 'action', 'documentary', 'adventure', 'western', 'animation', 'romantic', 'sci', 'fi', 'biography', 'fantasy', 'crime', 'musical', 'war', 'grotesque', 'spy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparation of the scores"
      ],
      "metadata": {
        "id": "aenowVqC5eVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, since for the IMDB dataset there are two different scores, we will just take the average of them both. In the FilmTV datase, we already have an average score. We take note that the meta-score is a 0 to 100 rating so we devide it by 10 before adding it."
      ],
      "metadata": {
        "id": "NUPhSIgm5fDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset == \"IMDB\":\n",
        "  print(\"IMDB score:\", data_clean[:3,5], \"\\nmeta score:\", data_clean[:3,6])\n",
        "  data_clean[:,4] = (data_clean[:,5] + data_clean[:,6] / 10.0) / 2.0\n",
        "  print(\"\\n\\nAverage score:\", data_clean[:3,4])\n"
      ],
      "metadata": {
        "id": "ELqYya-j5so5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we normalize our scores by dividing them by 10."
      ],
      "metadata": {
        "id": "P8EI7Sbo6kMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc_normalized = data_clean[:,4] / 10.0\n",
        "print(\"Normalized score:\", sc_normalized[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAldnSGs6ptW",
        "outputId": "96d8c2fd-0305-4578-ff18-32eaef52f2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized score: [0.7 0.65 0.6 0.35 0.49000000000000005 0.72 0.38 0.73 0.6599999999999999\n",
            " 0.43]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Separating the datas for training and testing"
      ],
      "metadata": {
        "id": "j31Cmpim5AkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will pad the data that we tokenized with each tokenized feature's max_length so that the array are homogeneous."
      ],
      "metadata": {
        "id": "UAyXMt2a-cD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ov_max_len = max(len(x) for x in ov_tokenized)\n",
        "ti_max_len = max(len(x) for x in ti_tokenized)\n",
        "ge_max_len = max(len(x) for x in ge_tokenized)\n",
        "\n",
        "print(\"Maximum length for the Overview for the\", dataset, \"dataset:\", ov_max_len)\n",
        "print(\"Maximum length for the Title for the\", dataset, \"dataset:\", ti_max_len)\n",
        "print(\"Maximum length for the Genre for the\", dataset, \"dataset:\", ge_max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lffSoY0-8Yh",
        "outputId": "7668b812-7d2e-449a-ee69-223233475f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length for the Overview for the FilmTV dataset: 322\n",
            "Maximum length for the Title for the FilmTV dataset: 13\n",
            "Maximum length for the Genre for the FilmTV dataset: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "ov_padded = pad_sequences(ov_tokenized, maxlen=ov_max_len)\n",
        "ti_padded = pad_sequences(ti_tokenized, maxlen=ti_max_len)\n",
        "ge_padded = pad_sequences(ge_tokenized, maxlen=ge_max_len)"
      ],
      "metadata": {
        "id": "BBr9tLPh-bmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we prepare the datas for the training of our model like so:\n",
        "\n",
        "\n",
        "80% of the rows of data will be used to train the model with using first only the Overview feature and IMDB rating to train and evaluate.\n",
        "\n",
        "20% of the rows to test our model accuracy, since the datas have been shuffled, there shouldn't an order bias."
      ],
      "metadata": {
        "id": "-o9TQSbgaJIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of values for the\", dataset, \"dataset:\", len(data_clean[:,0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0ZWGeoP8nPU",
        "outputId": "006e2ace-1c97-40d0-ad59-8b9555f4ac50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of values for the FilmTV dataset: 39742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb_train_data = int(0.8*len(data_clean[:,0]))\n",
        "\n",
        "x_train_overview = ov_padded[:nb_train_data,:]\n",
        "x_train_title = ti_padded[:nb_train_data,:]\n",
        "x_train_director = di_labelized[:nb_train_data]\n",
        "x_train_genre = ge_padded[:nb_train_data,:]\n",
        "y_train_score = sc_normalized[:nb_train_data]\n",
        "\n",
        "x_test_overview = ov_padded[nb_train_data:,:]\n",
        "x_test_title = ti_padded[nb_train_data:,:]\n",
        "x_test_director = di_labelized[nb_train_data:]\n",
        "x_test_genre = ge_padded[nb_train_data:,:]\n",
        "y_test_score = sc_normalized[nb_train_data:]\n",
        "\n",
        "print(\"Final data prepared for the\", dataset, \"dataset:\\n\\nOverview:\\n\", ov_padded[:3,:],\"\\nTitle\\n\", ti_padded[:3,:],\"\\nDirector\\n\" ,di_labelized[:3],\"\\nGenre\\n\" ,ge_padded[:3,:],\"\\nScore\\n\" ,sc_normalized[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h43T5bmtU2Tp",
        "outputId": "dfdae4d4-f20d-4095-b9eb-ef52d8551b89",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final data prepared for the FilmTV dataset:\n",
            "\n",
            "Overview:\n",
            " [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0 28433  9714    34   149    65   646\n",
            "     92   191   556    16   169  2223   878    13 10929   230   403   914\n",
            "     28    93   165   268    84 28433  8857  2430   745  3320   110   131\n",
            "      7   208    19   418   303  3929  2915  5181  1203    78]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0  2269    60  2043 18758\n",
            "   2447 20854 11427  1858   288 37378   342  2316    16   346 20854  3875\n",
            "     30   333    96  1197 37379   444  1249   875  2520    37]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0 18759 11428 37380     6   169  8114\n",
            "  10492  4570  2772   130    30  4317  2191   227   122   538  4571  2296\n",
            "    885     5  1077 10930   439   899  2881   870   436   227]] \n",
            "Title\n",
            " [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "  10099]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0   560\n",
            "  10100]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0   561\n",
            "   2678]] \n",
            "Director\n",
            " [10798  6033  4409] \n",
            "Genre\n",
            " [[ 0  0  3]\n",
            " [ 0  0  4]\n",
            " [ 0  0 18]] \n",
            "Score\n",
            " [0.7 0.65 0.6]\n"
          ]
        }
      ]
    }
  ]
}