{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpOz8PFvHZuB3/LC29Gv7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clemgi0/movie-analyser_deep-learning-proyecto/blob/main/03_arquitectura_de_linea_de_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DATASET\n",
        "https://www.kaggle.com/datasets/harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows"
      ],
      "metadata": {
        "id": "qsSfnx55I4y9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8SnR2BdxHVp6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import os\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows\")\n",
        "\n",
        "files_in_path = os.listdir(path)\n",
        "csv_files = [f for f in files_in_path if f.endswith('.csv')]\n",
        "\n",
        "if csv_files:\n",
        "    data_file = os.path.join(path, csv_files[0])\n",
        "    df = pd.read_csv(data_file)\n",
        "    data = df.to_numpy()\n",
        "    data = data[:, [1, 5, 7, 9, 6, 8]] # Name of the movie / Genre / Overview / Director / IMDB rating / meta-score\n",
        "    print(\"Data shape:\", data[:3,:])\n",
        "else:\n",
        "    print(\"No CSV files found in the specified path. Please specify which file to load if it's not a CSV or has a different extension.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSrWH8P7Hkb2",
        "outputId": "0923400b-5360-42d1-e3cc-5c4c95e17b51"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'imdb-dataset-of-top-1000-movies-and-tv-shows' dataset.\n",
            "Data shape: [['The Shawshank Redemption' 'Drama'\n",
            "  'Two imprisoned men bond over a number of years, finding solace and eventual redemption through acts of common decency.'\n",
            "  'Frank Darabont' 9.3 80.0]\n",
            " ['The Godfather' 'Crime, Drama'\n",
            "  \"An organized crime dynasty's aging patriarch transfers control of his clandestine empire to his reluctant son.\"\n",
            "  'Francis Ford Coppola' 9.2 100.0]\n",
            " ['The Dark Knight' 'Action, Crime, Drama'\n",
            "  'When the menace known as the Joker wreaks havoc and chaos on the people of Gotham, Batman must accept one of the greatest psychological and physical tests of his ability to fight injustice.'\n",
            "  'Christopher Nolan' 9.0 84.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "stopwords_en = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# Remove the stopwords from nltk english stopwords dictionnary\n",
        "cleaned_texts = np.array([])\n",
        "for text in data[:,2]:\n",
        "    tokens = [word.lower() for word in nltk.word_tokenize(text) if word.lower() not in stopwords_en]\n",
        "    cleaned_texts = np.append(cleaned_texts, ' '.join(tokens))\n",
        "\n",
        "# Tokenize the cleaned dataset of movie's resume\n",
        "max_features = 5000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(cleaned_texts)\n",
        "tokenizer.word_index.update({'<pad>': 0})\n",
        "X_cleaned = tokenizer.texts_to_sequences(cleaned_texts)\n",
        "\n",
        "# Convert each element of X_cleaned to a numpy array\n",
        "X_cleaned = np.array([np.array(x) for x in X_cleaned], dtype=object)\n",
        "\n",
        "# Retrieve the differents data's\n",
        "x_train = np.column_stack((data[:800, [0, 1, 3]], X_cleaned[:800])) # Name of the movie / Genre / Overview / Director\n",
        "y_train = data[:800, [4, 5]] # IMDB rating / meta-score\n",
        "\n",
        "x_test = np.column_stack((data[800:, [0, 1, 3]], X_cleaned[800:])) # Name of the movie / Genre / Overview / Director\n",
        "y_test = data[800:, [4, 5]] # IMDB rating / meta-score\n",
        "print(\"Shapes of x_train and x_test:\", x_train.shape, x_test.shape)\n",
        "print(\"\\n3 firsts samples of x_train:\", x_train[:3,:])\n",
        "print(\"\\n3 firsts samples of x_test:\", x_test[:3,:])\n",
        "print(\"\\nShapes of y_train and y_test:\", y_train.shape, y_test.shape)\n",
        "print(\"\\n3 firsts samples of y_train:\", y_train[:3,:])\n",
        "print(\"\\n3 firsts samples of y_test:\", y_test[:3,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gLXVE-T6HpzK",
        "outputId": "acb7c065-13df-4b0c-efe3-0974b4d981e9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of x_train and x_test: (800, 4) (200, 4)\n",
            "\n",
            "3 firsts samples of x_train: [['The Shawshank Redemption' 'Drama' 'Frank Darabont'\n",
            "  array([   5,  271,   48,  135, 1312,   25,  508, 1313, 2219,  272, 1314,\n",
            "          921, 2220])                                                     ]\n",
            " ['The Godfather' 'Crime, Drama' 'Francis Ford Coppola'\n",
            "  array([2221,   45, 2222,    1,  273, 1315, 2223,  922, 2224,  197,  923,\n",
            "           26])                                                           ]\n",
            " ['The Dark Knight' 'Action, Crime, Drama' 'Christopher Nolan'\n",
            "  array([2225,  413,  924, 1316, 1317, 1318,   46,  659,  660,   16, 2226,\n",
            "           13,  925, 1319,  414, 2227, 2228,   82, 2229])                 ]]\n",
            "\n",
            "3 firsts samples of x_test: [['Office Space' 'Comedy' 'Mike Judge'\n",
            "  array([  30,  448, 2177, 1162, 2046,  433,  470, 1766,  410])]\n",
            " ['Happiness' 'Comedy, Drama' 'Todd Solondz'\n",
            "  array([  32,  172, 1426, 1327,   90,   32,  900,  808, 1601, 1314,  242,\n",
            "         1152,  910,   14, 4966,  219,   54,  108,  545])                 ]\n",
            " ['Training Day' 'Crime, Drama, Thriller' 'Antoine Fuqua'\n",
            "  array([ 964,  289,  841,   61,   53,  189,  190, 2178,   95, 1751,   88,\n",
            "          118, 4967])                                                     ]]\n",
            "\n",
            "Shapes of y_train and y_test: (800, 2) (200, 2)\n",
            "\n",
            "3 firsts samples of y_train: [[9.3 80.0]\n",
            " [9.2 100.0]\n",
            " [9.0 84.0]]\n",
            "\n",
            "3 firsts samples of y_test: [[7.7 68.0]\n",
            " [7.7 81.0]\n",
            " [7.7 69.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uZwP9DtBKBoq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}