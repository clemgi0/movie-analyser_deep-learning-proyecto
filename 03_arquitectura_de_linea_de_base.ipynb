{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt/7BbaYTE53PogmkeqobF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clemgi0/movie-analyser_deep-learning-proyecto/blob/main/03_arquitectura_de_linea_de_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Movie Analyser | Deep Learning Final Project\n",
        "\n",
        "In this serie of notebook, we will follow my avances for this project. Let's begin by defining it. Basically, what I want to achieve is to create a deep learning IA model using Keras and Tensorflow that could predict the success of a movie through it's resume, and some other possible input datas like the name of the movie, it's director or it's genre."
      ],
      "metadata": {
        "id": "CDMOGTkEqTDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATASETS\n",
        "https://www.kaggle.com/datasets/harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows\n",
        "\n",
        "Features of the first dataset :\n",
        "\n",
        "Here are it's features:\n",
        "\n",
        "0 Poster_Link - Link of the poster that imdb using\n",
        "\n",
        "1 Series_Title - Name of the movie\n",
        "\n",
        "2 Released_Year - Year at which that movie released\n",
        "\n",
        "3 Certificate - Certificate earned by that movie\n",
        "\n",
        "4 Runtime - Total runtime of the movie\n",
        "\n",
        "5 Genre - Genre of the movie\n",
        "\n",
        "6 IMDB_Rating - Rating of the movie at IMDB site\n",
        "\n",
        "7 Overview - mini story/ summary\n",
        "\n",
        "8 Meta_score - Score earned by the movie\n",
        "\n",
        "9 Director - Name of the Director\n",
        "\n",
        "10, 11, 12, 13 Star1,Star2,Star3,Star4 - Name of the Stars\n",
        "\n",
        "14 No_of_votes - Total number of votes\n",
        "\n",
        "15 Gross - Money earned by that movie\n",
        "\n"
      ],
      "metadata": {
        "id": "qsSfnx55I4y9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/datasets/stefanoleone992/filmtv-movies-dataset\n",
        "\n",
        "Features of the second dataset :\n",
        "\n",
        "0 Filmtv_id - Movie id\n",
        "\n",
        "1 Title - Name of the movie\n",
        "\n",
        "2 Year - Movie year of release\n",
        "\n",
        "3 Genre - Movie genre\n",
        "\n",
        "4 Duration - Movie duration (in min)\n",
        "\n",
        "5 Country - Countries where the movie was filmed\n",
        "\n",
        "6 Directors - Name of movie directors\n",
        "\n",
        "7 Actors - Name of movie actors\n",
        "\n",
        "8 Avg_vote - Average rating (by critics and public)\n",
        "\n",
        "9 Critics_vote - Average vote of the critics\n",
        "\n",
        "10 Public_vote - Average vote of the public\n",
        "\n",
        "11 Total_vote - Total votes expressed by critics and public\n",
        "\n",
        "12 Overview - Movie description\n",
        "\n",
        "13 Notes - Movie notes\n",
        "\n",
        "14 Humor - Movie humor score given by filmtv\n",
        "\n",
        "15 Rythm - Movie rythm score given by filmtv\n",
        "\n",
        "16 Tension - Movie tension score given by filmtv\n",
        "\n",
        "17 Erotism - Movie erotism score given by filmtv"
      ],
      "metadata": {
        "id": "XL9Pg4z0qa9U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "8SnR2BdxHVp6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import os\n",
        "import nltk\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base architecture\n",
        "In this thirs notebook, we will focus on realising a basic architecture to test out our datas. Then, we will try to implement a more complicated model architecture to intent to get the best results possible."
      ],
      "metadata": {
        "id": "9u_46aFSqfVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we select the dataset that we want to use.**"
      ],
      "metadata": {
        "id": "GCub89cgq3-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows\")\n",
        "dataset = \"IMDB\""
      ],
      "metadata": {
        "id": "IHAxmaaJq2UG",
        "outputId": "04c2d876-1842-497f-aa5f-5d76aae2c9e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'imdb-dataset-of-top-1000-movies-and-tv-shows' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"stefanoleone992/filmtv-movies-dataset\")\n",
        "dataset = \"FilmTV\""
      ],
      "metadata": {
        "id": "_xcMCZ0Cq3l0",
        "outputId": "34b25d80-9aaf-4cd1-d30b-da56d89700c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'filmtv-movies-dataset' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we import the datas we want and shuffle them for the reason we saw on the first notebook. We also withdraw the features that interests us."
      ],
      "metadata": {
        "id": "Q9Y4MogZrpIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files_in_path = os.listdir(path)\n",
        "csv_files = [f for f in files_in_path if f.endswith('.csv')]\n",
        "\n",
        "if csv_files:\n",
        "    data_file = os.path.join(path, csv_files[0])\n",
        "    df = pd.read_csv(data_file)\n",
        "\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True) # Shuffle the datas to avoid linear IMDB rating\n",
        "\n",
        "    data = df.to_numpy()\n",
        "    if dataset == \"IMDB\":\n",
        "      data = data[:, [1, 5, 7, 9, 6, 8]] # Title / Genre / Overview / Director / IMDB rating / meta-score\n",
        "    else:\n",
        "      data = data[:, [1, 3, 12, 6, 9, 10]] # Title / Genre / Overview / Director / Critics votes / Public votes\n",
        "\n",
        "    print(\"Data shape:\", data[:3,:])\n",
        "else:\n",
        "    print(\"No CSV files found in the specified path. Please specify which file to load if it's not a CSV or has a different extension.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSrWH8P7Hkb2",
        "outputId": "ea7665a9-a5f0-45ce-b9c5-00c205a0a6ca",
        "collapsed": true
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: [['Svadba' 'Comedy'\n",
            "  'Mishka and Tania, friends since school, are getting married. But something is wrong because the girl leaves for Moscow and disappears for a few years. Having vanished the dreams of becoming a model, she decides to return to the country to marry the good Mishka, muscular and solid worker with a clean face, still in love with her. At this point the film tells about the wedding preparations, the ceremony, the dramas and the trafficking that goes through it.'\n",
            "  'Pavel Lungin' 7.0 7.0]\n",
            " ['The Phantom of Crestwood' 'Thriller'\n",
            "  'Pushed by the beautiful Jenny Wren, banker Priam Andes throws a party at Crestwood, his summer residence. The girl asks Priam to also invite three wealthy men whom she intends to pluck, but her plans will be unexpectedly upset by an inexplicable death ...'\n",
            "  'J. Walter Ruben' 6.0 7.0]\n",
            " ['Ragazzi della marina' 'War'\n",
            "  'The cruiser \"Raimondo Montecuccoli\" leaves Livorno with the cadets of the Naval Academy. Among them, three sailors are worried about problems of a different nature. The authority of the superiors and the common life on board improves the character of the crew, also managing to solve their personal problems.'\n",
            "  'Francesco De Robertis' 6.0 6.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we prepocess the datas as we did it in second notebook by cleaning, shortening, tokenizing and normalizing them."
      ],
      "metadata": {
        "id": "ORY1lvlxrqaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the rows containing nan values\n",
        "df2 = pd.DataFrame(data)\n",
        "df2 = df2.replace(\"nan\", np.nan)   # if \"nan\" is a chain, we delete the row\n",
        "df2 = df2.dropna()\n",
        "data_clean = df2.to_numpy()\n",
        "\n",
        "\n",
        "# Downloading the english stopwords dictionnary\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "stopwords_en = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "\n",
        "# Remove the stopwords from nltk english stopwords dictionnary to get a clean dataset\n",
        "cleaned_texts = []\n",
        "for text in data_clean[:,2]:\n",
        "    tokens = [word.lower() for word in nltk.word_tokenize(text) if word.lower() not in stopwords_en]\n",
        "    cleaned_texts.append(' '.join(tokens))\n",
        "\n",
        "\n",
        "# Tokenize the cleaned dataset of movie's Overview\n",
        "if dataset == \"IMDB\":\n",
        "  max_features = 5000\n",
        "else:\n",
        "  max_features = 50000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(cleaned_texts)\n",
        "tokenizer.word_index.update({'<pad>': 0})\n",
        "X_cleaned = tokenizer.texts_to_sequences(cleaned_texts)\n",
        "\n",
        "\n",
        "# Retrieve the differents data's\n",
        "if dataset == \"IMDB\":\n",
        "  #x_train = data[:800, [0, 1, 3]] # Name of the movie / Genre / Director\n",
        "  x_train = X_cleaned[:800] # Overview\n",
        "  y_train = data_clean[:800, [4, 5]] # IMDB rating / meta-score\n",
        "\n",
        "  #x_test = data[800:, [0, 1, 2, 3]] # Name of the movie / Genre / Director\n",
        "  x_test = X_cleaned[800:] # Overview\n",
        "  y_test = data_clean[800:, [4, 5]] # IMDB rating / meta-score\n",
        "\n",
        "  # Normalization of the goal's datas\n",
        "  y_train[:, 0] = y_train[:, 0] / 10.0   # IMDB rating\n",
        "  y_train[:, 1] = y_train[:, 1] / 100.0  # Meta-score\n",
        "\n",
        "  y_test[:, 0] = y_test[:, 0] / 10.0\n",
        "  y_test[:, 1] = y_test[:, 1] / 100.0\n",
        "else:\n",
        "  #x_train = data[:33000, [0, 1, 3]] # Name of the movie / Genre / Director\n",
        "  x_train = X_cleaned[:33000] # Overview\n",
        "  y_train = data_clean[:33000, [4, 5]] / 10.0 # IMDB rating / meta-score\n",
        "\n",
        "  #x_test = data[33000:, [0, 1, 2, 3]] # Name of the movie / Genre / Director\n",
        "  x_test = X_cleaned[33000:] # Overview\n",
        "  y_test = data_clean[33000:, [4, 5]] /10.0 # IMDB rating / meta-score\n",
        "\n",
        "print(\"\\nx_train :\\n\", x_train[:3], \"\\nx_test :\\n\", x_test[:3])\n",
        "if dataset == \"IMDB\":\n",
        "  print(\"\\ny_train :\\nIMDB rating | meta-score\\n\", y_train[:3,:], \"\\ny_test :\\nIMDB rating | meta-score\\n\",y_test[:3,:])\n",
        "else:\n",
        "  print(\"\\ny_train :\\nCritic votes | Public votes\\n\", y_train[:3,:], \"\\ny_test :\\nCritic votes | Public votes\\n\",y_test[:3,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gLXVE-T6HpzK",
        "outputId": "eadc740d-7a8a-4f1c-ee2f-d95b17fbc608"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "x_train :\n",
            " [[27083, 10282, 37, 166, 75, 687, 95, 210, 609, 15, 171, 2126, 901, 14, 10744, 251, 419, 915, 31, 104, 165, 270, 84, 27083, 8281, 2307, 718, 3548, 116, 138, 7, 196, 20, 404, 378, 4384, 2866, 4836, 1211, 81], [2322, 60, 2227, 17829, 2414, 19809, 10745, 1774, 318, 35621, 360, 2487, 15, 368, 19809, 16, 3791, 33, 322, 97, 1171, 35622, 505, 1442, 898, 2579, 40], [19810, 10746, 35623, 5, 171, 8282, 10747, 4524, 2716, 125, 33, 4189, 2286, 243, 135, 544, 4457, 2171, 953, 6, 1094, 10748, 402, 922, 16, 2867, 997, 459, 243]] \n",
            "x_test :\n",
            " [[5956, 8494, 23568, 68, 736, 1684, 5155, 333, 522, 13, 2088, 25, 157, 692, 4434, 18498, 297, 605, 242, 42, 4344, 592, 287], [4775, 1393, 2200, 68, 61, 37733, 13, 81, 15604, 1288, 35, 76, 42, 4, 8, 70, 336, 1831, 24, 153, 38, 524, 107, 658, 1543, 10810, 6248, 17, 7173, 8571, 27, 136, 948, 4413, 82, 7065, 2595, 6115], [2540, 20538, 182, 2462, 470, 12535, 1761, 1021, 271, 50, 3694, 295, 403, 367, 1154, 7092, 3214, 12535, 595, 384, 20538, 1265, 1682, 1783, 1612, 9760, 8893, 148, 1122, 167, 811, 2132, 21, 135, 2540, 144, 490, 13665, 188, 46, 3500, 1988, 6]]\n",
            "\n",
            "y_train :\n",
            "Critic votes | Public votes\n",
            " [[0.7 0.7]\n",
            " [0.6 0.7]\n",
            " [0.6 0.6]] \n",
            "y_test :\n",
            "Critic votes | Public votes\n",
            " [[0.631 0.6]\n",
            " [0.6 0.8]\n",
            " [0.567 0.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Maximum length of an Overview for the dataset chosen:\",max(len(x) for x in x_train))"
      ],
      "metadata": {
        "id": "9mjDxVdhnEho",
        "outputId": "8d3dd908-e49c-4326-a989-b815cc42a531",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length of an Overview for the dataset chosen: 209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we finish to prepare our datas that will train our model. For the IMDB dataset, the maximum length of an shortened overview is so we chose a padding sequence of 40 to be sure that we don't loose word and we follow the same logic for the FilmTV dataset"
      ],
      "metadata": {
        "id": "lLFywvnwsXES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "if dataset == \"IMDB\":\n",
        "  max_features = 5000\n",
        "else:\n",
        "  max_features = 50000\n",
        "embedding_dim = 64\n",
        "if dataset == \"IMDB\":\n",
        "  max_len = 40\n",
        "else:\n",
        "  max_len = 340\n",
        "\n",
        "X_train_seq = pad_sequences(x_train, maxlen=max_len)\n",
        "X_test_seq  = pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "y_train_reg = y_train[:, 0].astype(np.float32)\n",
        "y_test_reg  = y_test[:, 0].astype(np.float32)"
      ],
      "metadata": {
        "id": "uZwP9DtBKBoq"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we create our first model, something really simple with a GlobalAveragePooling1D layer, just to see what we can get from the preprocessing of our datas."
      ],
      "metadata": {
        "id": "WT6mikv_uEeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Architecture ---\n",
        "inputs = Input(shape=(max_len,))\n",
        "x = Embedding(max_features, embedding_dim)(inputs)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "DEGbVsdLXcDS"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "4z01DfefXeCL",
        "outputId": "6b1f4ad4-0f69-4bf7-afdf-aae1ae1ba7b2"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m340\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m340\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m3,200,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_6      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">340</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">340</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_6      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,204,225\u001b[0m (12.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,204,225</span> (12.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,204,225\u001b[0m (12.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,204,225</span> (12.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_seq, y_train_reg, epochs=15, batch_size=128, verbose=1)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkJKLvF4VtK7",
        "outputId": "9d8fd3f7-8615-48ec-b188-75ff5cab87e6"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 65ms/step - loss: 0.0267 - mae: 0.1330\n",
            "Epoch 2/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.0249 - mae: 0.1287\n",
            "Epoch 3/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0253 - mae: 0.1282\n",
            "Epoch 4/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0262 - mae: 0.1321\n",
            "Epoch 5/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0248 - mae: 0.1284\n",
            "Epoch 6/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0249 - mae: 0.1280\n",
            "Epoch 7/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0249 - mae: 0.1275\n",
            "Epoch 8/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0247 - mae: 0.1276\n",
            "Epoch 9/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0269 - mae: 0.1339\n",
            "Epoch 10/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 0.0254 - mae: 0.1287\n",
            "Epoch 11/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.0255 - mae: 0.1287\n",
            "Epoch 12/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0258 - mae: 0.1304\n",
            "Epoch 13/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0257 - mae: 0.1285\n",
            "Epoch 14/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0247 - mae: 0.1277\n",
            "Epoch 15/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0247 - mae: 0.1279\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7841b07854c0>"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, mae = model.evaluate(X_test_seq, y_test_reg, verbose=1)\n",
        "print(\"MSE (loss) :\", loss)\n",
        "print(\"MAE :\", mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7oYMsfUXDni",
        "outputId": "a2ce7264-29a5-401e-fcd2-459774ab80a4"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1006/1006\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0251 - mae: 0.1269\n",
            "MSE (loss) : 0.024928240105509758\n",
            "MAE : 0.126577690243721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can see that this model is doing a pretty miserable at predicting our movies score but we knew it would be like that since it can't encapsulate the sense of the Overview without an RNN layer. So, this is what our next model will include."
      ],
      "metadata": {
        "id": "hL8MCKinvFdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_seq)\n",
        "\n",
        "for i in range(10):\n",
        "    print(\"Overview:\", data[800+i, 2][:80], \"...\")\n",
        "    print(\"Real rating :\", y_test_reg[i], \" – Prediction :\", y_pred[i][0])\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOoM6FMuZKYU",
        "outputId": "83b45c19-5411-453a-dcbf-f28a43be31f1"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 71 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7841afbf1300> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1006/1006\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
            "Overview: Bill and Cassel, leaders of two rival gangs unjustly accused of murdering three  ...\n",
            "Real rating : 0.631  – Prediction : 0.5855587\n",
            "---\n",
            "Overview: Woody Grant (Bruce Dern), a short-tempered old man who has turned everyone away  ...\n",
            "Real rating : 0.6  – Prediction : 0.58299434\n",
            "---\n",
            "Overview: A road movie on a red van around Sicily in search of the new oral narrators who  ...\n",
            "Real rating : 0.567  – Prediction : 0.58289796\n",
            "---\n",
            "Overview: A young London woman arrives at an isolated castle to visit relatives she hasn't ...\n",
            "Real rating : 0.694  – Prediction : 0.58223045\n",
            "---\n",
            "Overview: The bloodthirsty count this time is a famous scientist who invented a virus capa ...\n",
            "Real rating : 0.667  – Prediction : 0.58130246\n",
            "---\n",
            "Overview: After thirty years spent in Geneva as master builder in the Boyer civil engineer ...\n",
            "Real rating : 0.333  – Prediction : 0.57736945\n",
            "---\n",
            "Overview: Nick Naylor's job is to represent the tobacco multinationals, and as a result he ...\n",
            "Real rating : 0.533  – Prediction : 0.58203655\n",
            "---\n",
            "Overview: Taken in Fiumicino by a shady individual in charge of taking him to France, the  ...\n",
            "Real rating : 0.8  – Prediction : 0.58722997\n",
            "---\n",
            "Overview: From the famous story by Frank Baum, the prodigious adventure of the young Dorot ...\n",
            "Real rating : 0.7  – Prediction : 0.58216417\n",
            "---\n",
            "Overview: A woman living in Mexico is sued by an American court: she is the main accusatio ...\n",
            "Real rating : 0.667  – Prediction : 0.58561176\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}